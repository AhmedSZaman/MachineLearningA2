{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4f463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class TrafficGenerator(gym.Env):\n",
    "    \n",
    "    #doesnt really workj\n",
    "    def getObs(self):\n",
    "        # Loop over the packet types\n",
    "        for i in range(len(self.mean_delay)):\n",
    "\n",
    "            # Get the current queue for the packet type\n",
    "            current_queue = self.queues[i]\n",
    "\n",
    "            # Calculate the length and average waiting time of the current queue\n",
    "            current_length = len(current_queue)\n",
    "            current_waiting_time = np.average(current_queue) if current_length > 0 else 0.0\n",
    "            #print(np.average(current_queue), current_length, current_waiting_time )\n",
    "            \n",
    "            \n",
    "           \n",
    "            observation.append([current_length, current_waiting_time])\n",
    "\n",
    "       \n",
    "        observation = np.array(observation)\n",
    "        return observation\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Define the observation space (number of packets in each queue and their waiting time)\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,2), dtype=int)\n",
    "        \n",
    "        # pancket info (DataType, arrival_rate, mean_delay )\n",
    "        #self.packetInfo = [[0, 0.3, 6],[1, 0.25, 4],[2, 0.4, float('inf')]]\n",
    "        \n",
    "        self.dataType = [0, 1, 2]\n",
    "        self.arrival_rate = [0.3, 0.25, 0.4]\n",
    "        self.mean_delay_req = [6, 4, float('inf')]\n",
    "        self.packetInfo = [[elem1, elem2, elem3] for elem1, elem2, elem3 in zip(self.dataType, self.arrival_rate, self.mean_delay_req)]\n",
    "        self.curr_mean_delay_best_effort = 0\n",
    "        self.packet = 1\n",
    "        self.timeslot = 1\n",
    "        self.totaltime = self.timeslot\n",
    "        \n",
    "        # Initialize the queues\n",
    "        self.queues = [[], [], []]\n",
    "        \n",
    "        self.current_queue = 0\n",
    "      \n",
    "    \n",
    "    def step(self, action):  \n",
    "        self.totaltime += self.timeslot\n",
    "        print(self.totaltime)\n",
    "        #packet generator\n",
    "        for i in range(len(self.packetInfo)):\n",
    "            for sublist in self.packetInfo:\n",
    "                if sublist[0] == i and np.random.uniform() < sublist[1]:\n",
    "                    self.queues[i].append(self.packet)\n",
    "                    #print(\"Appending to queue\", i)\n",
    "        \n",
    "        for i in range(len(self.queues)):\n",
    "            for j in range(len(self.queues[i])):\n",
    "                self.queues[i][j] -= self.timeslot\n",
    "                \n",
    "        \n",
    "        # services the queues\n",
    "        #for i in range(len(self.queues)):\n",
    "            #print(\"queue \", i, \": has a length of\", len(self.queues[i]))\n",
    "        #print(\"ACTION queue \", action, \": has a length of\", len(self.queues[action]))\n",
    "        if action == 0 and len(self.queues[self.current_queue]) > 0:\n",
    "            self.queues[self.current_queue].pop(0)\n",
    "            for i in range(len(self.queues[self.current_queue])):\n",
    "                self.queues[self.current_queue][i] -= 1\n",
    "        elif action == 1:\n",
    "            if self.current_queue == 0:\n",
    "                self.current_queue = 2\n",
    "            else:\n",
    "                self.current_queue -= 1\n",
    "        elif action == 2:\n",
    "            if self.current_queue == 2:\n",
    "                self.current_queue = 0\n",
    "            else:\n",
    "                self.current_queue += 1\n",
    "        \n",
    "        #TODO rewardoptions?\n",
    "        reward = 1\n",
    "        \n",
    "        observation = []\n",
    "        for i in range(len(self.mean_delay_req)):\n",
    "\n",
    "            # Get the current queue for the packet type\n",
    "            current_queue = self.queues[i]\n",
    "\n",
    "            # Calculate the length and average waiting time of the current queue\n",
    "            current_length = len(current_queue)\n",
    "            current_waiting_time = np.average(current_queue) if current_length > 0 else 0.0\n",
    "            #print(np.average(current_queue), current_length, current_waiting_time )\n",
    "            \n",
    "    \n",
    "            observation.append([current_length, current_waiting_time])\n",
    "            if i == 2:\n",
    "                #reward option\n",
    "                if self.curr_mean_delay_best_effort == 0:\n",
    "                    curr_mean_delay_best_effort = current_waiting_time\n",
    "                if self.curr_mean_delay_best_effort < current_waiting_time:\n",
    "                    self.curr_mean_delay_best_effort = current_waiting_time\n",
    "                    reward = 1\n",
    "                elif self.curr_mean_delay_best_effort > current_waiting_time:\n",
    "                    self.curr_mean_delay_best_effort = current_waiting_time\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 1\n",
    "        \n",
    "                #print(self.curr_mean_delay_best_effort)\n",
    "                #print(current_waiting_time)\n",
    "        print(f\"observation{observation}\")\n",
    "        if observation[0][1] < -4:\n",
    "            reward = -10\n",
    "        elif observation[1][1] < -6:\n",
    "            reward = -10\n",
    "        observation = np.array(observation)\n",
    "\n",
    "         # Loop over the packet types\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "        for i in range(len(self.mean_delay)):\n",
    "\n",
    "            # Get the current queue for the packet type\n",
    "            current_queue = self.queues[i]\n",
    "\n",
    "            # Calculate the length and average waiting time of the current queue\n",
    "            current_length = len(current_queue)\n",
    "            current_waiting_time = np.average(current_queue) if current_length > 0 else 0.0\n",
    "            #print(np.average(current_queue), current_length, current_waiting_time )\n",
    "            \n",
    "            \n",
    "    \n",
    "            observation.append([current_length, current_waiting_time])\n",
    "\n",
    "        observation = np.array(observation)\n",
    "        \"\"\"\n",
    "        info = {\"mean_delay_0\": self.mean_delay_req[0],\n",
    "        \"mean_delay_1\": self.mean_delay_req[1],\n",
    "        \"mean_delay_2\": self.mean_delay_req[2],\n",
    "        \"arrival_rate_0\": self.arrival_rate[0],\n",
    "        \"arrival_rate_1\": self.arrival_rate[1],\n",
    "        \"arrival_rate_2\": self.arrival_rate[2]}\n",
    "        done = len(self.queues[0]) + len(self.queues[1]) + len(self.queues[2]) == 0 or (self.totaltime >= 100)\n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the queues\n",
    "        self.queues = [[], [], []]\n",
    "        return np.array([[len(self.queues[i]), 0.0] for i in range(len(self.mean_delay_req))])\n",
    "        \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e03171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~\n",
      "[0. 0. 0.]\n",
      "chosen 2\n",
      "2\n",
      "observation[[0, 0.0], [0, 0.0], [0, 0.0]]\n",
      "reward 1\n",
      "~~~~~~~~~\n",
      "[0. 0. 0.]\n",
      "chosen 2\n",
      "3\n",
      "observation[[0, 0.0], [0, 0.0], [1, 0.0]]\n",
      "reward 1\n",
      "~~~~~~~~~\n",
      "[0. 0. 1.]\n",
      "chosen 2\n",
      "4\n",
      "observation[[0, 0.0], [0, 0.0], [2, -0.5]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 0. 2.]\n",
      "chosen 2\n",
      "5\n",
      "observation[[0, 0.0], [1, 0.0], [2, -1.5]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 1. 2.]\n",
      "chosen 1\n",
      "6\n",
      "observation[[0, 0.0], [1, -1.0], [2, -2.5]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 1. 2.]\n",
      "chosen 1\n",
      "7\n",
      "observation[[0, 0.0], [1, -2.0], [3, -2.3333333333333335]]\n",
      "reward 1\n",
      "~~~~~~~~~\n",
      "[0. 1. 3.]\n",
      "chosen 1\n",
      "8\n",
      "observation[[0, 0.0], [1, -3.0], [4, -2.5]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 1. 4.]\n",
      "chosen 1\n",
      "9\n",
      "observation[[0, 0.0], [2, -2.0], [4, -3.5]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 2. 4.]\n",
      "chosen 1\n",
      "10\n",
      "observation[[0, 0.0], [2, -3.0], [5, -3.6]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 2. 5.]\n",
      "chosen 1\n",
      "11\n",
      "observation[[0, 0.0], [2, -4.0], [5, -4.6]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 2. 5.]\n",
      "chosen 1\n",
      "12\n",
      "observation[[0, 0.0], [2, -5.0], [5, -5.6]]\n",
      "reward -1\n",
      "~~~~~~~~~\n",
      "[0. 2. 5.]\n",
      "chosen 1\n",
      "13\n",
      "observation[[0, 0.0], [2, -6.0], [6, -5.5]]\n",
      "reward 1\n",
      "~~~~~~~~~\n",
      "[0. 2. 6.]\n",
      "chosen 1\n",
      "14\n",
      "observation[[1, 0.0], [2, -7.0], [7, -5.571428571428571]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[1. 2. 7.]\n",
      "chosen 1\n",
      "15\n",
      "observation[[1, -1.0], [2, -8.0], [8, -5.75]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[1. 2. 8.]\n",
      "chosen 1\n",
      "16\n",
      "observation[[1, -2.0], [2, -9.0], [8, -6.75]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[1. 2. 8.]\n",
      "chosen 1\n",
      "17\n",
      "observation[[1, -3.0], [3, -6.666666666666667], [8, -7.75]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[1. 3. 8.]\n",
      "chosen 1\n",
      "18\n",
      "observation[[1, -4.0], [3, -7.666666666666667], [8, -8.75]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[1. 3. 8.]\n",
      "chosen 1\n",
      "19\n",
      "observation[[1, -5.0], [3, -8.666666666666666], [8, -9.75]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[1. 3. 8.]\n",
      "chosen 1\n",
      "20\n",
      "observation[[2, -3.0], [3, -9.666666666666666], [9, -9.555555555555555]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[2. 3. 9.]\n",
      "chosen 1\n",
      "21\n",
      "observation[[2, -4.0], [3, -10.666666666666666], [10, -9.5]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[ 2.  3. 10.]\n",
      "chosen 1\n",
      "22\n",
      "observation[[3, -3.3333333333333335], [3, -11.666666666666666], [11, -9.545454545454545]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[ 3.  3. 11.]\n",
      "chosen 0\n",
      "23\n",
      "observation[[4, -3.25], [3, -12.666666666666666], [11, -9.727272727272727]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[ 4.  3. 11.]\n",
      "chosen 0\n",
      "24\n",
      "observation[[4, -4.25], [3, -13.666666666666666], [10, -10.7]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[ 4.  3. 10.]\n",
      "chosen 0\n",
      "25\n",
      "observation[[5, -4.2], [4, -11.0], [9, -11.777777777777779]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[5. 4. 9.]\n",
      "chosen 0\n",
      "26\n",
      "observation[[6, -4.333333333333333], [5, -9.6], [8, -12.75]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[6. 5. 8.]\n",
      "chosen 0\n",
      "27\n",
      "observation[[6, -5.333333333333333], [5, -10.6], [8, -12.125]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[6. 5. 8.]\n",
      "chosen 0\n",
      "28\n",
      "observation[[7, -5.428571428571429], [5, -11.6], [7, -13.142857142857142]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 5. 7.]\n",
      "chosen 0\n",
      "29\n",
      "observation[[7, -6.428571428571429], [6, -10.5], [6, -14.0]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 6. 6.]\n",
      "chosen 0\n",
      "30\n",
      "observation[[7, -7.428571428571429], [6, -11.5], [6, -12.333333333333334]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 6. 6.]\n",
      "chosen 0\n",
      "31\n",
      "observation[[7, -8.428571428571429], [6, -12.5], [6, -11.166666666666666]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 6. 6.]\n",
      "chosen 0\n",
      "32\n",
      "observation[[7, -9.428571428571429], [6, -13.5], [6, -9.833333333333334]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 6. 6.]\n",
      "chosen 0\n",
      "33\n",
      "observation[[7, -10.428571428571429], [7, -12.428571428571429], [5, -9.8]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 7. 5.]\n",
      "chosen 0\n",
      "34\n",
      "observation[[7, -11.428571428571429], [8, -11.75], [5, -7.4]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 5.]\n",
      "chosen 1\n",
      "35\n",
      "observation[[7, -12.428571428571429], [8, -12.75], [5, -8.4]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 5.]\n",
      "chosen 1\n",
      "36\n",
      "observation[[7, -13.428571428571429], [8, -13.75], [6, -7.833333333333333]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 6.]\n",
      "chosen 1\n",
      "37\n",
      "observation[[7, -14.428571428571429], [8, -14.75], [6, -8.833333333333334]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 6.]\n",
      "chosen 1\n",
      "38\n",
      "observation[[7, -15.428571428571429], [8, -15.75], [7, -8.428571428571429]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 7.]\n",
      "chosen 1\n",
      "39\n",
      "observation[[7, -16.428571428571427], [8, -16.75], [7, -9.428571428571429]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 7.]\n",
      "chosen 1\n",
      "40\n",
      "observation[[7, -17.428571428571427], [8, -17.75], [7, -10.428571428571429]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 7.]\n",
      "chosen 1\n",
      "41\n",
      "observation[[7, -18.428571428571427], [8, -18.75], [7, -11.428571428571429]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 7.]\n",
      "chosen 1\n",
      "42\n",
      "observation[[7, -19.428571428571427], [8, -19.75], [8, -10.875]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[7. 8. 8.]\n",
      "chosen 1\n",
      "43\n",
      "observation[[8, -17.875], [9, -18.444444444444443], [9, -10.555555555555555]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[8. 9. 9.]\n",
      "chosen 1\n",
      "44\n",
      "observation[[9, -16.77777777777778], [10, -17.5], [9, -11.555555555555555]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[ 9. 10.  9.]\n",
      "chosen 1\n",
      "45\n",
      "observation[[10, -16.0], [11, -16.818181818181817], [9, -12.555555555555555]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[10. 11.  9.]\n",
      "chosen 1\n",
      "46\n",
      "observation[[11, -15.454545454545455], [12, -16.333333333333332], [10, -12.2]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[11. 12. 10.]\n",
      "chosen 1\n",
      "47\n",
      "observation[[11, -16.454545454545453], [13, -16.0], [10, -13.2]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[11. 13. 10.]\n",
      "chosen 1\n",
      "48\n",
      "observation[[12, -16.0], [13, -17.0], [10, -14.2]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[12. 13. 10.]\n",
      "chosen 1\n",
      "49\n",
      "observation[[12, -17.0], [14, -16.714285714285715], [10, -15.2]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[12. 14. 10.]\n",
      "chosen 1\n",
      "50\n",
      "observation[[12, -18.0], [14, -17.714285714285715], [11, -14.727272727272727]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[12. 14. 11.]\n",
      "chosen 1\n",
      "51\n",
      "observation[[12, -19.0], [14, -18.714285714285715], [12, -14.416666666666666]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[12. 14. 12.]\n",
      "chosen 1\n",
      "52\n",
      "observation[[12, -20.0], [15, -18.4], [12, -15.416666666666666]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[12. 15. 12.]\n",
      "chosen 1\n",
      "53\n",
      "observation[[12, -21.0], [15, -19.4], [12, -16.416666666666668]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[12. 15. 12.]\n",
      "chosen 1\n",
      "54\n",
      "observation[[13, -20.307692307692307], [15, -20.4], [12, -17.416666666666668]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[13. 15. 12.]\n",
      "chosen 1\n",
      "55\n",
      "observation[[13, -21.307692307692307], [15, -21.4], [12, -18.416666666666668]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[13. 15. 12.]\n",
      "chosen 1\n",
      "56\n",
      "observation[[13, -22.307692307692307], [15, -22.4], [12, -19.416666666666668]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[13. 15. 12.]\n",
      "chosen 1\n",
      "57\n",
      "observation[[13, -23.307692307692307], [15, -23.4], [12, -20.416666666666668]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[13. 15. 12.]\n",
      "chosen 1\n",
      "58\n",
      "observation[[13, -24.307692307692307], [15, -24.4], [13, -19.76923076923077]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[13. 15. 13.]\n",
      "chosen 1\n",
      "59\n",
      "observation[[14, -23.5], [15, -25.4], [14, -19.285714285714285]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 15. 14.]\n",
      "chosen 1\n",
      "60\n",
      "observation[[14, -24.5], [15, -26.4], [14, -20.285714285714285]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 15. 14.]\n",
      "chosen 1\n",
      "61\n",
      "observation[[14, -25.5], [16, -25.6875], [15, -19.866666666666667]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 16. 15.]\n",
      "chosen 1\n",
      "62\n",
      "observation[[14, -26.5], [16, -26.6875], [15, -20.866666666666667]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 16. 15.]\n",
      "chosen 1\n",
      "63\n",
      "observation[[14, -27.5], [17, -26.058823529411764], [16, -20.5]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 17. 16.]\n",
      "chosen 1\n",
      "64\n",
      "observation[[14, -28.5], [17, -27.058823529411764], [17, -20.235294117647058]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 17. 17.]\n",
      "chosen 1\n",
      "65\n",
      "observation[[14, -29.5], [17, -28.058823529411764], [17, -21.235294117647058]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 17. 17.]\n",
      "chosen 1\n",
      "66\n",
      "observation[[14, -30.5], [17, -29.058823529411764], [17, -22.235294117647058]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 17. 17.]\n",
      "chosen 1\n",
      "67\n",
      "observation[[14, -31.5], [18, -28.38888888888889], [17, -23.235294117647058]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 18. 17.]\n",
      "chosen 1\n",
      "68\n",
      "observation[[14, -32.5], [18, -29.38888888888889], [17, -24.235294117647058]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 18. 17.]\n",
      "chosen 1\n",
      "69\n",
      "observation[[14, -33.5], [19, -28.789473684210527], [17, -25.235294117647058]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 19. 17.]\n",
      "chosen 1\n",
      "70\n",
      "observation[[14, -34.5], [19, -29.789473684210527], [18, -24.77777777777778]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 19. 18.]\n",
      "chosen 1\n",
      "71\n",
      "observation[[14, -35.5], [20, -29.25], [19, -24.42105263157895]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 20. 19.]\n",
      "chosen 1\n",
      "72\n",
      "observation[[14, -36.5], [20, -30.25], [19, -25.42105263157895]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 20. 19.]\n",
      "chosen 1\n",
      "73\n",
      "observation[[14, -37.5], [21, -29.761904761904763], [20, -25.1]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[14. 21. 20.]\n",
      "chosen 1\n",
      "74\n",
      "observation[[15, -35.93333333333333], [21, -30.761904761904763], [21, -24.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[15. 21. 21.]\n",
      "chosen 1\n",
      "75\n",
      "observation[[15, -36.93333333333333], [21, -31.761904761904763], [21, -25.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[15. 21. 21.]\n",
      "chosen 1\n",
      "76\n",
      "observation[[16, -35.5625], [21, -32.76190476190476], [21, -26.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[16. 21. 21.]\n",
      "chosen 1\n",
      "77\n",
      "observation[[16, -36.5625], [22, -32.22727272727273], [21, -27.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[16. 22. 21.]\n",
      "chosen 1\n",
      "78\n",
      "observation[[16, -37.5625], [22, -33.22727272727273], [21, -28.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[16. 22. 21.]\n",
      "chosen 1\n",
      "79\n",
      "observation[[17, -36.294117647058826], [23, -32.73913043478261], [21, -29.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[17. 23. 21.]\n",
      "chosen 1\n",
      "80\n",
      "observation[[17, -37.294117647058826], [23, -33.73913043478261], [21, -30.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[17. 23. 21.]\n",
      "chosen 1\n",
      "81\n",
      "observation[[18, -36.166666666666664], [24, -33.291666666666664], [21, -31.857142857142858]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[18. 24. 21.]\n",
      "chosen 1\n",
      "82\n",
      "observation[[18, -37.166666666666664], [25, -32.92], [21, -32.857142857142854]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[18. 25. 21.]\n",
      "chosen 1\n",
      "83\n",
      "observation[[19, -36.1578947368421], [26, -32.61538461538461], [21, -33.857142857142854]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[19. 26. 21.]\n",
      "chosen 1\n",
      "84\n",
      "observation[[19, -37.1578947368421], [27, -32.370370370370374], [21, -34.857142857142854]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[19. 27. 21.]\n",
      "chosen 1\n",
      "85\n",
      "observation[[19, -38.1578947368421], [28, -32.17857142857143], [21, -35.857142857142854]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[19. 28. 21.]\n",
      "chosen 1\n",
      "86\n",
      "observation[[20, -37.2], [28, -33.17857142857143], [22, -35.18181818181818]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[20. 28. 22.]\n",
      "chosen 1\n",
      "87\n",
      "observation[[20, -38.2], [29, -33.0], [23, -34.608695652173914]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[20. 29. 23.]\n",
      "chosen 1\n",
      "88\n",
      "observation[[21, -37.333333333333336], [29, -34.0], [23, -35.608695652173914]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[21. 29. 23.]\n",
      "chosen 1\n",
      "89\n",
      "observation[[21, -38.333333333333336], [29, -35.0], [23, -36.608695652173914]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[21. 29. 23.]\n",
      "chosen 1\n",
      "90\n",
      "observation[[22, -37.54545454545455], [29, -36.0], [23, -37.608695652173914]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[22. 29. 23.]\n",
      "chosen 1\n",
      "91\n",
      "observation[[22, -38.54545454545455], [29, -37.0], [23, -38.608695652173914]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[22. 29. 23.]\n",
      "chosen 1\n",
      "92\n",
      "observation[[23, -37.82608695652174], [29, -38.0], [23, -39.608695652173914]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[23. 29. 23.]\n",
      "chosen 1\n",
      "93\n",
      "observation[[23, -38.82608695652174], [29, -39.0], [24, -38.916666666666664]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[23. 29. 24.]\n",
      "chosen 1\n",
      "94\n",
      "observation[[23, -39.82608695652174], [29, -40.0], [25, -38.32]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[23. 29. 25.]\n",
      "chosen 1\n",
      "95\n",
      "observation[[24, -39.125], [30, -39.63333333333333], [25, -39.32]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[24. 30. 25.]\n",
      "chosen 1\n",
      "96\n",
      "observation[[24, -40.125], [31, -39.32258064516129], [25, -40.32]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[24. 31. 25.]\n",
      "chosen 1\n",
      "97\n",
      "observation[[24, -41.125], [31, -40.32258064516129], [26, -39.73076923076923]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[24. 31. 26.]\n",
      "chosen 1\n",
      "98\n",
      "observation[[24, -42.125], [31, -41.32258064516129], [26, -40.73076923076923]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[24. 31. 26.]\n",
      "chosen 1\n",
      "99\n",
      "observation[[24, -43.125], [31, -42.32258064516129], [26, -41.73076923076923]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[24. 31. 26.]\n",
      "chosen 1\n",
      "100\n",
      "observation[[25, -42.36], [31, -43.32258064516129], [26, -42.73076923076923]]\n",
      "reward -10\n",
      "~~~~~~~~~\n",
      "[25. 31. 26.]\n",
      "chosen 1\n",
      "101\n",
      "observation[[25, -43.36], [31, -44.32258064516129], [26, -43.73076923076923]]\n",
      "reward -10\n"
     ]
    }
   ],
   "source": [
    "env = TrafficGenerator()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "x = 0\n",
    "while x < 100:\n",
    "    print(\"~~~~~~~~~\")\n",
    "    print(obs[:, 0])\n",
    "    \n",
    "    if obs[:, 0][0] > 0 or obs[:, 0][1] > 0:\n",
    "        action = np.argmax(obs[:2, 0])\n",
    "    else:\n",
    "        action = 2 \n",
    "    \n",
    "    print(\"chosen\", action)\n",
    "    obs,reward, done, info = env.step(action)\n",
    "    print(\"reward\", reward)\n",
    "    \n",
    "    x = x +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd905ec0",
   "metadata": {},
   "source": [
    "# Training the DQN agent dude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6875fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1924fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states = env.observation_space.shape\n",
    "states = (1,3,2)\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93aa9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential() \n",
    "    model.add(Flatten(input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    print(states)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef5559f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1ed7cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "381a8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f7749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4824e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions, states):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c601b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "Keras version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ddd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76ce2091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "113\n",
      "observation[[1, 0.0], [0, 0.0], [0, 0.0]]\n",
      "    1/10000 [..............................] - ETA: 30:05 - reward: 1.0000114\n",
      "observation[[0, 0.0], [0, 0.0], [0, 0.0]]\n",
      "115\n",
      "observation[[0, 0.0], [0, 0.0], [0, 0.0]]\n",
      "116\n",
      "observation[[1, 0.0], [0, 0.0], [0, 0.0]]\n",
      "117\n",
      "observation[[1, 0.0], [0, 0.0], [0, 0.0]]\n",
      "118\n",
      "observation[[1, 0.0], [0, 0.0], [0, 0.0]]\n",
      "119\n",
      "observation[[1, 0.0], [0, 0.0], [0, 0.0]]\n",
      "120\n",
      "observation[[0, 0.0], [0, 0.0], [0, 0.0]]\n",
      "121\n",
      "observation[[1, 0.0], [0, 0.0], [1, 0.0]]\n",
      "122\n",
      "observation[[0, 0.0], [0, 0.0], [0, 0.0]]\n",
      "123\n",
      "observation[[0, 0.0], [0, 0.0], [0, 0.0]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute 'get_updates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m dqn \u001b[38;5;241m=\u001b[39m build_agent(model, actions, states)\n\u001b[0;32m      4\u001b[0m dqn\u001b[38;5;241m.\u001b[39mcompile(optimizer, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rl\\core.py:215\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# We are in a terminal state but the agent hasn't yet seen it. We therefore\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# perform one more forward-backward call and simply ignore the action before\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# resetting the environment. We need to pass in `terminal=False` here since\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# the *next* state, that is the state of the newly reset environment, is\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# always non-terminal by convention.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(observation)\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# This episode is finished, report and reset.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     episode_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m'\u001b[39m: episode_reward,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_episode_steps\u001b[39m\u001b[38;5;124m'\u001b[39m: episode_step,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_steps\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep,\n\u001b[0;32m    222\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rl\\agents\\dqn.py:321\u001b[0m, in \u001b[0;36mDQNAgent.backward\u001b[1;34m(self, reward, terminal)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Finally, perform a single update on the entire batch. We use a dummy target since\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# the actual loss is computed in a Lambda layer that needs more complex input. However,\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# it is still useful to know the actual target to compute metrics properly.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m ins \u001b[38;5;241m=\u001b[39m [state0_batch] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m state0_batch\n\u001b[1;32m--> 321\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdummy_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [metric \u001b[38;5;28;01mfor\u001b[39;00m idx, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics) \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)]  \u001b[38;5;66;03m# throw away individual losses\u001b[39;00m\n\u001b[0;32m    323\u001b[0m metrics \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:1180\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1177\u001b[0m         ins \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m]  \u001b[38;5;66;03m# Add learning phase value.\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_sample_weight_modes(sample_weights\u001b[38;5;241m=\u001b[39msample_weights)\n\u001b[1;32m-> 1180\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_train_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(ins)\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_metrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:2284\u001b[0m, in \u001b[0;36mModel._make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m   2282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2283\u001b[0m         \u001b[38;5;66;03m# Training updates\u001b[39;00m\n\u001b[1;32m-> 2284\u001b[0m         updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_updates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2285\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collected_trainable_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2287\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2288\u001b[0m         \u001b[38;5;66;03m# Unconditional updates\u001b[39;00m\n\u001b[0;32m   2289\u001b[0m         updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_updates_for(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rl\\util.py:91\u001b[0m, in \u001b[0;36mAdditionalUpdatesOptimizer.get_updates\u001b[1;34m(self, params, loss)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_updates\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, loss):\n\u001b[1;32m---> 91\u001b[0m     updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_updates\u001b[49m(params\u001b[38;5;241m=\u001b[39mparams, loss\u001b[38;5;241m=\u001b[39mloss)\n\u001b[0;32m     92\u001b[0m     updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_updates\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdates \u001b[38;5;241m=\u001b[39m updates\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'get_updates'"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=1e-3)\n",
    "Adam._name = \"adam\"\n",
    "dqn = build_agent(model, actions, states)\n",
    "dqn.compile(optimizer, metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=1000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783c72ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "1102\n",
      "1\n",
      "Episode 1: reward: 1.000, steps: 1\n",
      "1103\n",
      "1\n",
      "Episode 2: reward: 1.000, steps: 1\n",
      "1104\n",
      "1\n",
      "Episode 3: reward: 1.000, steps: 1\n",
      "1105\n",
      "1\n",
      "Episode 4: reward: 1.000, steps: 1\n",
      "1106\n",
      "1\n",
      "Episode 5: reward: 1.000, steps: 1\n",
      "1107\n",
      "1\n",
      "Episode 6: reward: 1.000, steps: 1\n",
      "1108\n",
      "1\n",
      "Episode 7: reward: 1.000, steps: 1\n",
      "1109\n",
      "1\n",
      "Episode 8: reward: 1.000, steps: 1\n",
      "1110\n",
      "1\n",
      "Episode 9: reward: 1.000, steps: 1\n",
      "1111\n",
      "1\n",
      "Episode 10: reward: 1.000, steps: 1\n",
      "1112\n",
      "1\n",
      "Episode 11: reward: 1.000, steps: 1\n",
      "1113\n",
      "1\n",
      "Episode 12: reward: 1.000, steps: 1\n",
      "1114\n",
      "1\n",
      "Episode 13: reward: 1.000, steps: 1\n",
      "1115\n",
      "1\n",
      "Episode 14: reward: 1.000, steps: 1\n",
      "1116\n",
      "1\n",
      "Episode 15: reward: 1.000, steps: 1\n",
      "1117\n",
      "1\n",
      "Episode 16: reward: 1.000, steps: 1\n",
      "1118\n",
      "1\n",
      "Episode 17: reward: 1.000, steps: 1\n",
      "1119\n",
      "1\n",
      "Episode 18: reward: 1.000, steps: 1\n",
      "1120\n",
      "1\n",
      "Episode 19: reward: 1.000, steps: 1\n",
      "1121\n",
      "1\n",
      "Episode 20: reward: 1.000, steps: 1\n",
      "1122\n",
      "1\n",
      "Episode 21: reward: 1.000, steps: 1\n",
      "1123\n",
      "1\n",
      "Episode 22: reward: 1.000, steps: 1\n",
      "1124\n",
      "1\n",
      "Episode 23: reward: 1.000, steps: 1\n",
      "1125\n",
      "1\n",
      "Episode 24: reward: 1.000, steps: 1\n",
      "1126\n",
      "1\n",
      "Episode 25: reward: 1.000, steps: 1\n",
      "1127\n",
      "1\n",
      "Episode 26: reward: 1.000, steps: 1\n",
      "1128\n",
      "1\n",
      "Episode 27: reward: 1.000, steps: 1\n",
      "1129\n",
      "1\n",
      "Episode 28: reward: 1.000, steps: 1\n",
      "1130\n",
      "1\n",
      "Episode 29: reward: 1.000, steps: 1\n",
      "1131\n",
      "1\n",
      "Episode 30: reward: 1.000, steps: 1\n",
      "1132\n",
      "1\n",
      "Episode 31: reward: 1.000, steps: 1\n",
      "1133\n",
      "1\n",
      "Episode 32: reward: 1.000, steps: 1\n",
      "1134\n",
      "1\n",
      "Episode 33: reward: 1.000, steps: 1\n",
      "1135\n",
      "1\n",
      "Episode 34: reward: 1.000, steps: 1\n",
      "1136\n",
      "1\n",
      "Episode 35: reward: 1.000, steps: 1\n",
      "1137\n",
      "1\n",
      "Episode 36: reward: 1.000, steps: 1\n",
      "1138\n",
      "1\n",
      "Episode 37: reward: 1.000, steps: 1\n",
      "1139\n",
      "1\n",
      "Episode 38: reward: 1.000, steps: 1\n",
      "1140\n",
      "1\n",
      "Episode 39: reward: 1.000, steps: 1\n",
      "1141\n",
      "1\n",
      "Episode 40: reward: 1.000, steps: 1\n",
      "1142\n",
      "1\n",
      "Episode 41: reward: 1.000, steps: 1\n",
      "1143\n",
      "1\n",
      "Episode 42: reward: 1.000, steps: 1\n",
      "1144\n",
      "1\n",
      "Episode 43: reward: 1.000, steps: 1\n",
      "1145\n",
      "1\n",
      "Episode 44: reward: 1.000, steps: 1\n",
      "1146\n",
      "1\n",
      "Episode 45: reward: 1.000, steps: 1\n",
      "1147\n",
      "1\n",
      "Episode 46: reward: 1.000, steps: 1\n",
      "1148\n",
      "1\n",
      "Episode 47: reward: 1.000, steps: 1\n",
      "1149\n",
      "1\n",
      "Episode 48: reward: 1.000, steps: 1\n",
      "1150\n",
      "1\n",
      "Episode 49: reward: 1.000, steps: 1\n",
      "1151\n",
      "1\n",
      "Episode 50: reward: 1.000, steps: 1\n",
      "1152\n",
      "1\n",
      "Episode 51: reward: 1.000, steps: 1\n",
      "1153\n",
      "1\n",
      "Episode 52: reward: 1.000, steps: 1\n",
      "1154\n",
      "1\n",
      "Episode 53: reward: 1.000, steps: 1\n",
      "1155\n",
      "1\n",
      "Episode 54: reward: 1.000, steps: 1\n",
      "1156\n",
      "1\n",
      "Episode 55: reward: 1.000, steps: 1\n",
      "1157\n",
      "1\n",
      "Episode 56: reward: 1.000, steps: 1\n",
      "1158\n",
      "1\n",
      "Episode 57: reward: 1.000, steps: 1\n",
      "1159\n",
      "1\n",
      "Episode 58: reward: 1.000, steps: 1\n",
      "1160\n",
      "1\n",
      "Episode 59: reward: 1.000, steps: 1\n",
      "1161\n",
      "1\n",
      "Episode 60: reward: 1.000, steps: 1\n",
      "1162\n",
      "1\n",
      "Episode 61: reward: 1.000, steps: 1\n",
      "1163\n",
      "1\n",
      "Episode 62: reward: 1.000, steps: 1\n",
      "1164\n",
      "1\n",
      "Episode 63: reward: 1.000, steps: 1\n",
      "1165\n",
      "1\n",
      "Episode 64: reward: 1.000, steps: 1\n",
      "1166\n",
      "1\n",
      "Episode 65: reward: 1.000, steps: 1\n",
      "1167\n",
      "1\n",
      "Episode 66: reward: 1.000, steps: 1\n",
      "1168\n",
      "1\n",
      "Episode 67: reward: 1.000, steps: 1\n",
      "1169\n",
      "1\n",
      "Episode 68: reward: 1.000, steps: 1\n",
      "1170\n",
      "1\n",
      "Episode 69: reward: 1.000, steps: 1\n",
      "1171\n",
      "1\n",
      "Episode 70: reward: 1.000, steps: 1\n",
      "1172\n",
      "1\n",
      "Episode 71: reward: 1.000, steps: 1\n",
      "1173\n",
      "1\n",
      "Episode 72: reward: 1.000, steps: 1\n",
      "1174\n",
      "1\n",
      "Episode 73: reward: 1.000, steps: 1\n",
      "1175\n",
      "1\n",
      "Episode 74: reward: 1.000, steps: 1\n",
      "1176\n",
      "1\n",
      "Episode 75: reward: 1.000, steps: 1\n",
      "1177\n",
      "1\n",
      "Episode 76: reward: 1.000, steps: 1\n",
      "1178\n",
      "1\n",
      "Episode 77: reward: 1.000, steps: 1\n",
      "1179\n",
      "1\n",
      "Episode 78: reward: 1.000, steps: 1\n",
      "1180\n",
      "1\n",
      "Episode 79: reward: 1.000, steps: 1\n",
      "1181\n",
      "1\n",
      "Episode 80: reward: 1.000, steps: 1\n",
      "1182\n",
      "1\n",
      "Episode 81: reward: 1.000, steps: 1\n",
      "1183\n",
      "1\n",
      "Episode 82: reward: 1.000, steps: 1\n",
      "1184\n",
      "1\n",
      "Episode 83: reward: 1.000, steps: 1\n",
      "1185\n",
      "1\n",
      "Episode 84: reward: 1.000, steps: 1\n",
      "1186\n",
      "1\n",
      "Episode 85: reward: 1.000, steps: 1\n",
      "1187\n",
      "1\n",
      "Episode 86: reward: 1.000, steps: 1\n",
      "1188\n",
      "1\n",
      "Episode 87: reward: 1.000, steps: 1\n",
      "1189\n",
      "1\n",
      "Episode 88: reward: 1.000, steps: 1\n",
      "1190\n",
      "1\n",
      "Episode 89: reward: 1.000, steps: 1\n",
      "1191\n",
      "1\n",
      "Episode 90: reward: 1.000, steps: 1\n",
      "1192\n",
      "1\n",
      "Episode 91: reward: 1.000, steps: 1\n",
      "1193\n",
      "1\n",
      "Episode 92: reward: 1.000, steps: 1\n",
      "1194\n",
      "1\n",
      "Episode 93: reward: 1.000, steps: 1\n",
      "1195\n",
      "1\n",
      "Episode 94: reward: 1.000, steps: 1\n",
      "1196\n",
      "1\n",
      "Episode 95: reward: 1.000, steps: 1\n",
      "1197\n",
      "1\n",
      "Episode 96: reward: 1.000, steps: 1\n",
      "1198\n",
      "1\n",
      "Episode 97: reward: 1.000, steps: 1\n",
      "1199\n",
      "1\n",
      "Episode 98: reward: 1.000, steps: 1\n",
      "1200\n",
      "1\n",
      "Episode 99: reward: 1.000, steps: 1\n",
      "1201\n",
      "1\n",
      "Episode 100: reward: 1.000, steps: 1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84198316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e864c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 6)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                168       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 843\n",
      "Trainable params: 843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd1a94f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6288c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
